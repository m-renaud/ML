<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
<head>
<!-- Generated by HsColour, http://code.haskell.org/~malcolm/hscolour/ -->
<title>src/ML/NN.hs</title>
<link type='text/css' rel='stylesheet' href='hscolour.css' />
</head>
<body>
<pre><a name="line-1"></a><span class='hs-comment'>{-# LANGUAGE BangPatterns #-}</span>
<a name="line-2"></a><span class='hs-comment'>{- |
<a name="line-3"></a>Simple neural network implementation, for learning purposes only.
<a name="line-4"></a>-}</span>
<a name="line-5"></a><span class='hs-keyword'>module</span> <span class='hs-conid'>ML</span><span class='hs-varop'>.</span><span class='hs-conid'>NN</span>
<a name="line-6"></a>       <span class='hs-layout'>(</span>
<a name="line-7"></a>           <span class='hs-comment'>-- * Data types</span>
<a name="line-8"></a>           <span class='hs-conid'>ActivationFunction</span>  <span class='hs-comment'>-- Re-export from NN.ActivationFunction.</span>
<a name="line-9"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>ActivationFunctionDerivative</span>  <span class='hs-comment'>-- Re-export from NN.ActivationFunction.</span>
<a name="line-10"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>Layer</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span>
<a name="line-11"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>Network</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span>
<a name="line-12"></a>
<a name="line-13"></a>           <span class='hs-comment'>-- * Network initialization</span>
<a name="line-14"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>randLayer</span>
<a name="line-15"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>randNetwork</span>
<a name="line-16"></a>
<a name="line-17"></a>           <span class='hs-comment'>-- * Running networks</span>
<a name="line-18"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>feedForward</span>
<a name="line-19"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>runNetwork</span>
<a name="line-20"></a>
<a name="line-21"></a>           <span class='hs-comment'>-- * Training networks</span>
<a name="line-22"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>TrainingConfig</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span>
<a name="line-23"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>Sample</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span>
<a name="line-24"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>Gradient</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span>
<a name="line-25"></a>       <span class='hs-layout'>,</span>   <span class='hs-conid'>CostDerivative</span>
<a name="line-26"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>sgd</span>
<a name="line-27"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>gradientDescentCore</span>
<a name="line-28"></a>
<a name="line-29"></a>           <span class='hs-comment'>-- * Cost function derivatives</span>
<a name="line-30"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>mse'</span>
<a name="line-31"></a>
<a name="line-32"></a>           <span class='hs-comment'>-- * Internal (exposed for testing)</span>
<a name="line-33"></a>       <span class='hs-layout'>,</span>   <span class='hs-varid'>computeZsAndAs</span>
<a name="line-34"></a>       <span class='hs-layout'>)</span> <span class='hs-keyword'>where</span>
<a name="line-35"></a>
<a name="line-36"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>ML</span><span class='hs-varop'>.</span><span class='hs-conid'>NN</span><span class='hs-varop'>.</span><span class='hs-conid'>ActivationFunction</span> <span class='hs-layout'>(</span><span class='hs-conid'>ActivationFunction</span><span class='hs-layout'>,</span> <span class='hs-conid'>ActivationFunctionDerivative</span><span class='hs-layout'>)</span>
<a name="line-37"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>ML</span><span class='hs-varop'>.</span><span class='hs-conid'>Sample</span> <span class='hs-layout'>(</span><span class='hs-conid'>Sample</span><span class='hs-layout'>(</span><span class='hs-keyglyph'>..</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-38"></a>
<a name="line-39"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Control</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span> <span class='hs-layout'>(</span><span class='hs-varid'>forM</span><span class='hs-layout'>,</span> <span class='hs-varid'>replicateM</span><span class='hs-layout'>)</span>
<a name="line-40"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Control</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span><span class='hs-varop'>.</span><span class='hs-conid'>Random</span> <span class='hs-layout'>(</span><span class='hs-conid'>Rand</span><span class='hs-layout'>,</span> <span class='hs-varid'>evalRandIO</span><span class='hs-layout'>,</span> <span class='hs-varid'>liftRand</span><span class='hs-layout'>)</span>
<a name="line-41"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Control</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span><span class='hs-varop'>.</span><span class='hs-conid'>Random</span><span class='hs-varop'>.</span><span class='hs-conid'>Extended</span> <span class='hs-layout'>(</span><span class='hs-varid'>vshuffle</span><span class='hs-layout'>)</span>
<a name="line-42"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Control</span><span class='hs-varop'>.</span><span class='hs-conid'>Monad</span><span class='hs-varop'>.</span><span class='hs-conid'>State</span> <span class='hs-layout'>(</span><span class='hs-conid'>State</span><span class='hs-layout'>,</span> <span class='hs-varid'>evalState</span><span class='hs-layout'>,</span> <span class='hs-varid'>get</span><span class='hs-layout'>,</span> <span class='hs-varid'>put</span><span class='hs-layout'>)</span>
<a name="line-43"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>Foldable</span> <span class='hs-layout'>(</span><span class='hs-varid'>foldl'</span><span class='hs-layout'>)</span>
<a name="line-44"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>List</span> <span class='hs-layout'>(</span><span class='hs-varid'>unfoldr</span><span class='hs-layout'>)</span>
<a name="line-45"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>Random</span><span class='hs-varop'>.</span><span class='hs-conid'>Normal</span> <span class='hs-layout'>(</span><span class='hs-varid'>normal</span><span class='hs-layout'>)</span>
<a name="line-46"></a><span class='hs-keyword'>import</span> <span class='hs-keyword'>qualified</span> <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-keyword'>as</span> <span class='hs-conid'>V</span>
<a name="line-47"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Numeric</span><span class='hs-varop'>.</span><span class='hs-conid'>LinearAlgebra</span> <span class='hs-layout'>(</span><span class='hs-conid'>Matrix</span><span class='hs-layout'>,</span> <span class='hs-conid'>R</span><span class='hs-layout'>,</span> <span class='hs-conid'>Vector</span><span class='hs-layout'>,</span> <span class='hs-layout'>(</span><span class='hs-varop'>&gt;&lt;</span><span class='hs-layout'>)</span><span class='hs-layout'>,</span> <span class='hs-layout'>(</span><span class='hs-varop'>&lt;&gt;</span><span class='hs-layout'>)</span><span class='hs-layout'>,</span> <span class='hs-layout'>(</span><span class='hs-cpp'>#&gt;</span><span class='hs-layout'>)</span><span class='hs-layout'>,</span> <span class='hs-varid'>cmap</span><span class='hs-layout'>,</span> <span class='hs-varid'>konst</span><span class='hs-layout'>,</span> <span class='hs-varid'>tr</span><span class='hs-layout'>,</span> <span class='hs-varid'>vector</span><span class='hs-layout'>)</span>
<a name="line-48"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>Numeric</span><span class='hs-varop'>.</span><span class='hs-conid'>LinearAlgebra</span><span class='hs-varop'>.</span><span class='hs-conid'>Data</span> <span class='hs-layout'>(</span><span class='hs-varid'>asColumn</span><span class='hs-layout'>,</span> <span class='hs-varid'>asRow</span><span class='hs-layout'>,</span> <span class='hs-varid'>size</span><span class='hs-layout'>)</span>
<a name="line-49"></a><span class='hs-keyword'>import</span>           <span class='hs-conid'>System</span><span class='hs-varop'>.</span><span class='hs-conid'>Random</span> <span class='hs-layout'>(</span><span class='hs-conid'>RandomGen</span><span class='hs-layout'>)</span>
<a name="line-50"></a>
<a name="line-51"></a>
<a name="line-52"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-53"></a><span class='hs-comment'>-- Types.</span>
<a name="line-54"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-55"></a>
<a name="line-56"></a><a name="Layer"></a><span class='hs-comment'>-- | A layer in a neural network is a bias vector and weight matrix.</span>
<a name="line-57"></a><a name="Layer"></a><span class='hs-comment'>--</span>
<a name="line-58"></a><a name="Layer"></a><span class='hs-comment'>-- Let /n/ be the number of neurons in the layer and /m/ be the number</span>
<a name="line-59"></a><a name="Layer"></a><span class='hs-comment'>-- of inputs to the layer. Then 'layerBiases' is a vector in Rⁿ and</span>
<a name="line-60"></a><a name="Layer"></a><span class='hs-comment'>-- 'layerWeights' is an /nxm/ real matrix.</span>
<a name="line-61"></a><a name="Layer"></a><span class='hs-keyword'>data</span> <span class='hs-conid'>Layer</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Layer</span>
<a name="line-62"></a>             <span class='hs-layout'>{</span> <span class='hs-varid'>layerBiases</span>  <span class='hs-keyglyph'>::</span> <span class='hs-comment'>{-# UNPACK #-}</span> <span class='hs-varop'>!</span><span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span>
<a name="line-63"></a>                                              <span class='hs-comment'>-- ^ A vector in Rⁿ representing the neuron biases.</span>
<a name="line-64"></a>             <span class='hs-layout'>,</span> <span class='hs-varid'>layerWeights</span> <span class='hs-keyglyph'>::</span> <span class='hs-comment'>{-# UNPACK #-}</span> <span class='hs-varop'>!</span><span class='hs-layout'>(</span><span class='hs-conid'>Matrix</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span>
<a name="line-65"></a>                                              <span class='hs-comment'>-- ^ An /nxm/ real matrix of the neuron weights.</span>
<a name="line-66"></a>             <span class='hs-layout'>}</span> <span class='hs-keyword'>deriving</span> <span class='hs-layout'>(</span><span class='hs-conid'>Show</span><span class='hs-layout'>,</span> <span class='hs-conid'>Read</span><span class='hs-layout'>)</span>
<a name="line-67"></a>
<a name="line-68"></a><a name="Network"></a><span class='hs-comment'>-- | A network is a list of layers.</span>
<a name="line-69"></a><a name="Network"></a><span class='hs-keyword'>data</span> <span class='hs-conid'>Network</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Network</span>
<a name="line-70"></a>               <span class='hs-layout'>{</span> <span class='hs-varid'>networkLayers</span> <span class='hs-keyglyph'>::</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Layer</span><span class='hs-keyglyph'>]</span>
<a name="line-71"></a>               <span class='hs-layout'>}</span> <span class='hs-keyword'>deriving</span> <span class='hs-layout'>(</span><span class='hs-conid'>Show</span><span class='hs-layout'>,</span> <span class='hs-conid'>Read</span><span class='hs-layout'>)</span>
<a name="line-72"></a>
<a name="line-73"></a>
<a name="line-74"></a><a name="TrainingConfig"></a><span class='hs-comment'>-- | Configuration for training a network.</span>
<a name="line-75"></a><a name="TrainingConfig"></a><span class='hs-keyword'>data</span> <span class='hs-conid'>TrainingConfig</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>TrainingConfig</span>
<a name="line-76"></a>                      <span class='hs-layout'>{</span> <span class='hs-varid'>trainingEta</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>R</span>  <span class='hs-comment'>-- ^ η - Learning rate.</span>
<a name="line-77"></a>                      <span class='hs-layout'>,</span> <span class='hs-varid'>trainingActivation</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunction</span>
<a name="line-78"></a>                      <span class='hs-layout'>,</span> <span class='hs-varid'>trainingActivationDerivative</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunctionDerivative</span>
<a name="line-79"></a>                      <span class='hs-layout'>,</span> <span class='hs-varid'>trainingCostDerivative</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>CostDerivative</span>
<a name="line-80"></a>                      <span class='hs-layout'>}</span>
<a name="line-81"></a>
<a name="line-82"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-83"></a><span class='hs-comment'>-- Network initialization.</span>
<a name="line-84"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-85"></a>
<a name="line-86"></a><a name="randNetwork"></a><span class='hs-comment'>-- | Generate a random neural network given the size of each layer.</span>
<a name="line-87"></a><span class='hs-comment'>--</span>
<a name="line-88"></a><span class='hs-comment'>-- For example, @[3,2,4]@ will generate a neural network with 3 input</span>
<a name="line-89"></a><span class='hs-comment'>-- neurons, 1 hidden layer with 2 neurons and 4 output neurons.</span>
<a name="line-90"></a><span class='hs-definition'>randNetwork</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>RandomGen</span> <span class='hs-varid'>g</span> <span class='hs-keyglyph'>=&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Int</span><span class='hs-keyglyph'>]</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Rand</span> <span class='hs-varid'>g</span> <span class='hs-conid'>Network</span>
<a name="line-91"></a><span class='hs-definition'>randNetwork</span> <span class='hs-varid'>sizes</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Network</span> <span class='hs-varop'>&lt;$&gt;</span> <span class='hs-varid'>forM</span> <span class='hs-varid'>dims</span> <span class='hs-layout'>(</span><span class='hs-varid'>uncurry</span> <span class='hs-varid'>randLayer</span><span class='hs-layout'>)</span>
<a name="line-92"></a>    <span class='hs-keyword'>where</span> <span class='hs-varid'>dims</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>adjacentPairs</span> <span class='hs-varid'>sizes</span>
<a name="line-93"></a>
<a name="line-94"></a><a name="adjacentPairs"></a><span class='hs-comment'>-- | Return a list of pairs of adjacent elements in a list.</span>
<a name="line-95"></a><span class='hs-comment'>--</span>
<a name="line-96"></a><span class='hs-comment'>-- Example: adjacentPairs [1,2,3] ==&gt; [(1,2), (2,3)]</span>
<a name="line-97"></a><span class='hs-definition'>adjacentPairs</span> <span class='hs-keyglyph'>::</span> <span class='hs-keyglyph'>[</span><span class='hs-varid'>a</span><span class='hs-keyglyph'>]</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-layout'>(</span><span class='hs-varid'>a</span><span class='hs-layout'>,</span><span class='hs-varid'>a</span><span class='hs-layout'>)</span><span class='hs-keyglyph'>]</span>
<a name="line-98"></a><span class='hs-definition'>adjacentPairs</span> <span class='hs-varid'>xs</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>zip</span> <span class='hs-layout'>(</span><span class='hs-varid'>init</span> <span class='hs-varid'>xs</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-varid'>tail</span> <span class='hs-varid'>xs</span><span class='hs-layout'>)</span>
<a name="line-99"></a>
<a name="line-100"></a><a name="randLayer"></a><span class='hs-comment'>-- | Generate a randomly initialized layer in a neural network.</span>
<a name="line-101"></a><span class='hs-definition'>randLayer</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>RandomGen</span> <span class='hs-varid'>g</span>
<a name="line-102"></a>             <span class='hs-keyglyph'>=&gt;</span> <span class='hs-conid'>Int</span>           <span class='hs-comment'>-- ^ Number of inputs into the layer.</span>
<a name="line-103"></a>             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Int</span>           <span class='hs-comment'>-- ^ Number of neurons in the layer.</span>
<a name="line-104"></a>             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Rand</span> <span class='hs-varid'>g</span> <span class='hs-conid'>Layer</span>  <span class='hs-comment'>-- ^ Randomly generated layer.</span>
<a name="line-105"></a><span class='hs-definition'>randLayer</span> <span class='hs-varid'>numInputs</span> <span class='hs-varid'>numNeurons</span> <span class='hs-keyglyph'>=</span> <span class='hs-keyword'>do</span>
<a name="line-106"></a>    <span class='hs-varid'>bias</span> <span class='hs-keyglyph'>&lt;-</span> <span class='hs-varid'>vector</span> <span class='hs-varop'>&lt;$&gt;</span> <span class='hs-varid'>replicateM</span> <span class='hs-varid'>numNeurons</span> <span class='hs-layout'>(</span><span class='hs-varid'>liftRand</span> <span class='hs-varid'>normal</span><span class='hs-layout'>)</span>
<a name="line-107"></a>    <span class='hs-varid'>weights</span> <span class='hs-keyglyph'>&lt;-</span> <span class='hs-layout'>(</span><span class='hs-varid'>numNeurons</span><span class='hs-varop'>&gt;&lt;</span><span class='hs-varid'>numInputs</span><span class='hs-layout'>)</span> <span class='hs-varop'>&lt;$&gt;</span> <span class='hs-varid'>replicateM</span> <span class='hs-layout'>(</span><span class='hs-varid'>numNeurons</span><span class='hs-varop'>*</span><span class='hs-varid'>numInputs</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-varid'>liftRand</span> <span class='hs-varid'>normal</span><span class='hs-layout'>)</span>
<a name="line-108"></a>    <span class='hs-varid'>return</span> <span class='hs-varop'>$</span> <span class='hs-conid'>Layer</span> <span class='hs-varid'>bias</span> <span class='hs-varid'>weights</span>
<a name="line-109"></a>
<a name="line-110"></a>
<a name="line-111"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-112"></a><span class='hs-comment'>-- Running networks.</span>
<a name="line-113"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-114"></a>
<a name="line-115"></a><a name="feedForward"></a><span class='hs-comment'>-- | Feed the output from a previous layer to the next layer.</span>
<a name="line-116"></a><span class='hs-definition'>feedForward</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunction</span>  <span class='hs-comment'>-- ^ Neuron activation function.</span>
<a name="line-117"></a>            <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>            <span class='hs-comment'>-- ^ Input to the layer.</span>
<a name="line-118"></a>            <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Layer</span>               <span class='hs-comment'>-- ^ Layer to run.</span>
<a name="line-119"></a>            <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>            <span class='hs-comment'>-- ^ Output from the layer.</span>
<a name="line-120"></a><span class='hs-definition'>feedForward</span> <span class='hs-varid'>act</span> <span class='hs-varid'>x</span> <span class='hs-layout'>(</span><span class='hs-conid'>Layer</span> <span class='hs-varid'>biases</span> <span class='hs-varid'>weights</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>cmap</span> <span class='hs-varid'>act</span> <span class='hs-varid'>z</span>
<a name="line-121"></a>    <span class='hs-keyword'>where</span> <span class='hs-varid'>z</span> <span class='hs-keyglyph'>=</span> <span class='hs-layout'>(</span><span class='hs-varid'>weights</span> <span class='hs-cpp'>#&gt;</span> <span class='hs-varid'>x</span><span class='hs-layout'>)</span> <span class='hs-varop'>+</span> <span class='hs-varid'>biases</span>
<a name="line-122"></a>
<a name="line-123"></a><a name="runNetwork"></a><span class='hs-comment'>-- | Run a neural network.</span>
<a name="line-124"></a><span class='hs-definition'>runNetwork</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Network</span>             <span class='hs-comment'>-- ^ Network to run.</span>
<a name="line-125"></a>           <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>ActivationFunction</span>  <span class='hs-comment'>-- ^ Neuron activation function.</span>
<a name="line-126"></a>           <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>            <span class='hs-comment'>-- ^ Network input.</span>
<a name="line-127"></a>           <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>            <span class='hs-comment'>-- ^ Network output.</span>
<a name="line-128"></a><span class='hs-definition'>runNetwork</span> <span class='hs-layout'>(</span><span class='hs-conid'>Network</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-varid'>act</span> <span class='hs-varid'>input</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>foldl'</span> <span class='hs-layout'>(</span><span class='hs-varid'>feedForward</span> <span class='hs-varid'>act</span><span class='hs-layout'>)</span> <span class='hs-varid'>input</span> <span class='hs-varid'>layers</span>
<a name="line-129"></a>
<a name="line-130"></a>
<a name="line-131"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-132"></a><span class='hs-comment'>-- Training networks.</span>
<a name="line-133"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-134"></a>
<a name="line-135"></a><a name="CostDerivative"></a><span class='hs-comment'>-- | A vectorized function which returns ∂Cₓ/∂a.</span>
<a name="line-136"></a><a name="CostDerivative"></a><span class='hs-comment'>--</span>
<a name="line-137"></a><a name="CostDerivative"></a><span class='hs-comment'>-- The first parameter is the output activation, the second parameter</span>
<a name="line-138"></a><a name="CostDerivative"></a><span class='hs-comment'>-- is the expected output.</span>
<a name="line-139"></a><a name="CostDerivative"></a><span class='hs-keyword'>type</span> <span class='hs-conid'>CostDerivative</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>
<a name="line-140"></a>
<a name="line-141"></a><a name="Gradient"></a><span class='hs-comment'>-- | The gradient of the network.</span>
<a name="line-142"></a><a name="Gradient"></a><span class='hs-keyword'>data</span> <span class='hs-conid'>Gradient</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Gradient</span>
<a name="line-143"></a>                <span class='hs-layout'>{</span> <span class='hs-varid'>gradientNablaB</span> <span class='hs-keyglyph'>::</span> <span class='hs-comment'>{-# UNPACK #-}</span> <span class='hs-varop'>!</span><span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-144"></a>                <span class='hs-layout'>,</span> <span class='hs-varid'>gradientNablaW</span> <span class='hs-keyglyph'>::</span> <span class='hs-comment'>{-# UNPACK #-}</span> <span class='hs-varop'>!</span><span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-layout'>(</span><span class='hs-conid'>Matrix</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-145"></a>                <span class='hs-layout'>}</span> <span class='hs-keyword'>deriving</span> <span class='hs-layout'>(</span><span class='hs-conid'>Read</span><span class='hs-layout'>,</span> <span class='hs-conid'>Show</span><span class='hs-layout'>)</span>
<a name="line-146"></a>
<a name="line-147"></a><a name="sumGradients"></a><span class='hs-definition'>sumGradients</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>Gradient</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Gradient</span>
<a name="line-148"></a><span class='hs-definition'>sumGradients</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>foldl1'</span> <span class='hs-varid'>addGradient</span>
<a name="line-149"></a>    <span class='hs-keyword'>where</span> <span class='hs-layout'>(</span><span class='hs-conid'>Gradient</span> <span class='hs-varop'>!</span><span class='hs-varid'>lb</span> <span class='hs-varop'>!</span><span class='hs-varid'>lw</span><span class='hs-layout'>)</span> <span class='hs-varop'>`addGradient`</span> <span class='hs-layout'>(</span><span class='hs-conid'>Gradient</span> <span class='hs-varop'>!</span><span class='hs-varid'>rb</span> <span class='hs-varop'>!</span><span class='hs-varid'>rw</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span>
<a name="line-150"></a>              <span class='hs-conid'>Gradient</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>zipWith</span> <span class='hs-layout'>(</span><span class='hs-varop'>+</span><span class='hs-layout'>)</span> <span class='hs-varid'>lb</span> <span class='hs-varid'>rb</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>zipWith</span> <span class='hs-layout'>(</span><span class='hs-varop'>+</span><span class='hs-layout'>)</span> <span class='hs-varid'>lw</span> <span class='hs-varid'>rw</span><span class='hs-layout'>)</span>
<a name="line-151"></a>
<a name="line-152"></a><a name="computeZsAndAs"></a><span class='hs-comment'>-- | Return z and activation values for each layer in the network.</span>
<a name="line-153"></a><span class='hs-comment'>--</span>
<a name="line-154"></a><span class='hs-comment'>-- The values are returned in reverse order for use by the</span>
<a name="line-155"></a><span class='hs-comment'>-- backpropogation algorithm.  We the State monad so it's more</span>
<a name="line-156"></a><span class='hs-comment'>-- explicit that the output activation of one layer is the input to</span>
<a name="line-157"></a><span class='hs-comment'>-- the next.</span>
<a name="line-158"></a><span class='hs-definition'>computeZsAndAs</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunction</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Network</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-layout'>(</span><span class='hs-keyglyph'>[</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-keyglyph'>]</span><span class='hs-layout'>,</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-keyglyph'>]</span><span class='hs-layout'>)</span>
<a name="line-159"></a><span class='hs-definition'>computeZsAndAs</span> <span class='hs-varid'>act</span> <span class='hs-layout'>(</span><span class='hs-conid'>Network</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-varid'>input</span> <span class='hs-keyglyph'>=</span> <span class='hs-layout'>(</span><span class='hs-varid'>reverse</span> <span class='hs-varid'>zs</span><span class='hs-layout'>,</span> <span class='hs-varid'>reverse</span> <span class='hs-layout'>(</span><span class='hs-varid'>input</span><span class='hs-conop'>:</span><span class='hs-keyword'>as</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>
<a name="line-160"></a>    <span class='hs-keyword'>where</span> <span class='hs-layout'>(</span><span class='hs-varid'>zs</span><span class='hs-layout'>,</span> <span class='hs-keyword'>as</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>unzip</span> <span class='hs-varop'>$</span> <span class='hs-varid'>evalState</span> <span class='hs-layout'>(</span><span class='hs-varid'>mapM</span> <span class='hs-varid'>computeZA</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-varid'>input</span>
<a name="line-161"></a>
<a name="line-162"></a>          <span class='hs-varid'>computeZA</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Layer</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>State</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>,</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span>
<a name="line-163"></a>          <span class='hs-varid'>computeZA</span> <span class='hs-layout'>(</span><span class='hs-conid'>Layer</span> <span class='hs-varid'>b</span> <span class='hs-varid'>w</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span> <span class='hs-keyword'>do</span>
<a name="line-164"></a>              <span class='hs-varid'>x</span> <span class='hs-keyglyph'>&lt;-</span> <span class='hs-varid'>get</span>
<a name="line-165"></a>              <span class='hs-keyword'>let</span> <span class='hs-varid'>z</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>w</span> <span class='hs-cpp'>#&gt;</span> <span class='hs-varid'>x</span> <span class='hs-varop'>+</span> <span class='hs-varid'>b</span>
<a name="line-166"></a>                  <span class='hs-varid'>a</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>cmap</span> <span class='hs-varid'>act</span> <span class='hs-varid'>z</span>
<a name="line-167"></a>              <span class='hs-varid'>put</span> <span class='hs-varid'>a</span>
<a name="line-168"></a>              <span class='hs-varid'>pure</span> <span class='hs-varop'>$</span> <span class='hs-layout'>(</span><span class='hs-varid'>z</span><span class='hs-layout'>,</span><span class='hs-varid'>a</span><span class='hs-layout'>)</span>
<a name="line-169"></a>
<a name="line-170"></a><a name="backpropogation"></a><span class='hs-comment'>-- | Compute the gradient for the training example.</span>
<a name="line-171"></a><span class='hs-definition'>backpropogation</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunction</span>
<a name="line-172"></a>                <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>ActivationFunctionDerivative</span>
<a name="line-173"></a>                <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>CostDerivative</span>
<a name="line-174"></a>                <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Network</span>
<a name="line-175"></a>                <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Sample</span>
<a name="line-176"></a>                <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Gradient</span>
<a name="line-177"></a><span class='hs-definition'>backpropogation</span> <span class='hs-varid'>act</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>cost'</span> <span class='hs-varid'>net</span><span class='hs-keyglyph'>@</span><span class='hs-layout'>(</span><span class='hs-conid'>Network</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>Sample</span> <span class='hs-varid'>x</span> <span class='hs-varid'>y</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span>
<a name="line-178"></a>    <span class='hs-conid'>Gradient</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>reverse</span> <span class='hs-varop'>$</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>cons</span> <span class='hs-varid'>nablaB_L</span> <span class='hs-varid'>nablaBs</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>reverse</span> <span class='hs-varop'>$</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>cons</span> <span class='hs-varid'>nablaW_L</span> <span class='hs-varid'>nablaWs</span><span class='hs-layout'>)</span>
<a name="line-179"></a>    <span class='hs-keyword'>where</span> <span class='hs-layout'>(</span><span class='hs-varid'>z</span><span class='hs-conop'>:</span><span class='hs-varid'>zs</span><span class='hs-layout'>,</span> <span class='hs-varid'>a</span><span class='hs-conop'>:</span><span class='hs-varid'>a'</span><span class='hs-conop'>:</span><span class='hs-keyword'>as</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>computeZsAndAs</span> <span class='hs-varid'>act</span> <span class='hs-varid'>net</span> <span class='hs-varid'>x</span>
<a name="line-180"></a>          <span class='hs-varid'>delta_L</span>  <span class='hs-keyglyph'>=</span> <span class='hs-varid'>cost'</span> <span class='hs-varid'>a</span> <span class='hs-varid'>y</span> <span class='hs-varop'>*</span> <span class='hs-varid'>cmap</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>z</span>
<a name="line-181"></a>          <span class='hs-varid'>nablaB_L</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>delta_L</span>
<a name="line-182"></a>          <span class='hs-varid'>nablaW_L</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>asColumn</span> <span class='hs-varid'>delta_L</span> <span class='hs-varop'>&lt;&gt;</span> <span class='hs-varid'>asRow</span> <span class='hs-varid'>a'</span>
<a name="line-183"></a>          <span class='hs-layout'>(</span><span class='hs-varid'>nablaBs</span><span class='hs-layout'>,</span> <span class='hs-varid'>nablaWs</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span>
<a name="line-184"></a>              <span class='hs-varid'>backpropogationBackwardsPass</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>delta_L</span> <span class='hs-varid'>zs</span> <span class='hs-keyword'>as</span> <span class='hs-layout'>(</span><span class='hs-varid'>reverse</span> <span class='hs-varop'>$</span> <span class='hs-varid'>tail</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span>
<a name="line-185"></a>
<a name="line-186"></a><a name="backpropogationBackwardsPass"></a><span class='hs-comment'>-- | Perform the backwards pass of the backpropogation algorithm.</span>
<a name="line-187"></a><span class='hs-comment'>--</span>
<a name="line-188"></a><span class='hs-comment'>-- Traverse the previously reverse z values, activations, and layers</span>
<a name="line-189"></a><span class='hs-comment'>-- from layer L to 2.  We thread the delta value along as state</span>
<a name="line-190"></a><span class='hs-definition'>backpropogationBackwardsPass</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>ActivationFunctionDerivative</span>
<a name="line-191"></a>                             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span>    <span class='hs-comment'>-- ^ delta for the last layer.</span>
<a name="line-192"></a>                             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-keyglyph'>]</span>  <span class='hs-comment'>-- ^ zs in reverse order.</span>
<a name="line-193"></a>                             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-keyglyph'>]</span>  <span class='hs-comment'>-- ^ activations in reverse order.</span>
<a name="line-194"></a>                             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>Layer</span><span class='hs-keyglyph'>]</span>     <span class='hs-comment'>-- ^ Layers L to 2.</span>
<a name="line-195"></a>                             <span class='hs-keyglyph'>-&gt;</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span><span class='hs-layout'>,</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-layout'>(</span><span class='hs-conid'>Matrix</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span>  <span class='hs-comment'>-- ^</span>
<a name="line-196"></a><span class='hs-definition'>backpropogationBackwardsPass</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>delta_L</span> <span class='hs-varid'>zs</span> <span class='hs-keyword'>as</span> <span class='hs-varid'>layers</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>unzip</span> <span class='hs-varid'>output</span>
<a name="line-197"></a>    <span class='hs-keyword'>where</span> <span class='hs-varop'>!</span><span class='hs-varid'>output</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>fromList</span> <span class='hs-varop'>$</span> <span class='hs-varid'>evalState</span> <span class='hs-layout'>(</span><span class='hs-varid'>mapM</span> <span class='hs-varid'>processLayer</span> <span class='hs-layout'>(</span><span class='hs-varid'>zip3</span> <span class='hs-varid'>layers</span> <span class='hs-varid'>zs</span> <span class='hs-keyword'>as</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span> <span class='hs-varid'>delta_L</span>
<a name="line-198"></a>
<a name="line-199"></a>          <span class='hs-varid'>processLayer</span> <span class='hs-keyglyph'>::</span> <span class='hs-layout'>(</span><span class='hs-conid'>Layer</span><span class='hs-layout'>,</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>,</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>State</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span><span class='hs-layout'>,</span> <span class='hs-conid'>Matrix</span> <span class='hs-conid'>R</span><span class='hs-layout'>)</span>
<a name="line-200"></a>          <span class='hs-varid'>processLayer</span> <span class='hs-layout'>(</span><span class='hs-layout'>(</span><span class='hs-conid'>Layer</span> <span class='hs-sel'>_bias</span> <span class='hs-varid'>weights</span><span class='hs-layout'>)</span><span class='hs-layout'>,</span> <span class='hs-varid'>z</span><span class='hs-layout'>,</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span> <span class='hs-keyword'>do</span>
<a name="line-201"></a>              <span class='hs-varid'>delta</span> <span class='hs-keyglyph'>&lt;-</span> <span class='hs-varid'>get</span>
<a name="line-202"></a>              <span class='hs-keyword'>let</span> <span class='hs-varid'>actPrime</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>cmap</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>z</span>
<a name="line-203"></a>                  <span class='hs-varid'>delta'</span>   <span class='hs-keyglyph'>=</span> <span class='hs-layout'>(</span><span class='hs-varid'>tr</span> <span class='hs-varid'>weights</span> <span class='hs-cpp'>#&gt;</span> <span class='hs-varid'>delta</span><span class='hs-layout'>)</span> <span class='hs-varop'>*</span> <span class='hs-varid'>actPrime</span>
<a name="line-204"></a>              <span class='hs-varid'>put</span> <span class='hs-varid'>delta'</span>
<a name="line-205"></a>              <span class='hs-varid'>pure</span> <span class='hs-layout'>(</span><span class='hs-varid'>delta'</span><span class='hs-layout'>,</span> <span class='hs-varid'>asColumn</span> <span class='hs-varid'>delta'</span> <span class='hs-varop'>&lt;&gt;</span> <span class='hs-varid'>asRow</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span>
<a name="line-206"></a>
<a name="line-207"></a><a name="gradientDescentCore"></a><span class='hs-comment'>-- | Update the network's weights and biases by applying gradient</span>
<a name="line-208"></a><span class='hs-comment'>--   descent for the given sample input.</span>
<a name="line-209"></a><span class='hs-definition'>gradientDescentCore</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>TrainingConfig</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>Sample</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Network</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Network</span>
<a name="line-210"></a><span class='hs-definition'>gradientDescentCore</span> <span class='hs-layout'>(</span><span class='hs-conid'>TrainingConfig</span> <span class='hs-varid'>eta</span> <span class='hs-varid'>act</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>cost'</span><span class='hs-layout'>)</span> <span class='hs-varid'>trainingData</span> <span class='hs-varid'>net</span><span class='hs-keyglyph'>@</span><span class='hs-layout'>(</span><span class='hs-conid'>Network</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-keyglyph'>=</span>
<a name="line-211"></a>    <span class='hs-conid'>Network</span> <span class='hs-varid'>layers'</span>
<a name="line-212"></a>    <span class='hs-keyword'>where</span> <span class='hs-varid'>sampleGradients</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>Gradient</span>
<a name="line-213"></a>          <span class='hs-varop'>!</span><span class='hs-varid'>sampleGradients</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>map</span> <span class='hs-layout'>(</span><span class='hs-varid'>backpropogation</span> <span class='hs-varid'>act</span> <span class='hs-varid'>act'</span> <span class='hs-varid'>cost'</span> <span class='hs-varid'>net</span><span class='hs-layout'>)</span> <span class='hs-varid'>trainingData</span>
<a name="line-214"></a>
<a name="line-215"></a>          <span class='hs-conid'>Gradient</span> <span class='hs-varid'>nablaB</span> <span class='hs-varid'>nablaW</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>sumGradients</span> <span class='hs-varid'>sampleGradients</span>
<a name="line-216"></a>
<a name="line-217"></a>          <span class='hs-varid'>numSamples</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>fromIntegral</span> <span class='hs-varop'>$</span> <span class='hs-varid'>length</span> <span class='hs-varid'>trainingData</span>
<a name="line-218"></a>          <span class='hs-varid'>layers'</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>toList</span> <span class='hs-varop'>$</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>zipWith3</span> <span class='hs-varid'>updateWeightsAndBiases</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>fromList</span> <span class='hs-varid'>layers</span><span class='hs-layout'>)</span> <span class='hs-varid'>nablaB</span> <span class='hs-varid'>nablaW</span>
<a name="line-219"></a>
<a name="line-220"></a>          <span class='hs-varid'>updateWeightsAndBiases</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Layer</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Vector</span> <span class='hs-conid'>R</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Matrix</span> <span class='hs-conid'>R</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Layer</span>
<a name="line-221"></a>          <span class='hs-varid'>updateWeightsAndBiases</span> <span class='hs-layout'>(</span><span class='hs-conid'>Layer</span> <span class='hs-varid'>b</span> <span class='hs-varid'>w</span><span class='hs-layout'>)</span> <span class='hs-varid'>nb</span> <span class='hs-varid'>nw</span> <span class='hs-keyglyph'>=</span>
<a name="line-222"></a>              <span class='hs-conid'>Layer</span> <span class='hs-layout'>(</span><span class='hs-varid'>b</span><span class='hs-comment'>-</span><span class='hs-layout'>(</span><span class='hs-varid'>konst</span> <span class='hs-layout'>(</span><span class='hs-varid'>eta</span><span class='hs-varop'>/</span><span class='hs-varid'>numSamples</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-varid'>size</span> <span class='hs-varid'>b</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span> <span class='hs-varop'>*</span> <span class='hs-varid'>nb</span><span class='hs-layout'>)</span>
<a name="line-223"></a>                    <span class='hs-layout'>(</span><span class='hs-varid'>w</span><span class='hs-comment'>-</span><span class='hs-layout'>(</span><span class='hs-varid'>konst</span> <span class='hs-layout'>(</span><span class='hs-varid'>eta</span><span class='hs-varop'>/</span><span class='hs-varid'>numSamples</span><span class='hs-layout'>)</span> <span class='hs-layout'>(</span><span class='hs-varid'>size</span> <span class='hs-varid'>w</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span> <span class='hs-varop'>*</span> <span class='hs-varid'>nw</span><span class='hs-layout'>)</span>
<a name="line-224"></a>
<a name="line-225"></a><a name="sgd"></a><span class='hs-comment'>-- | Train the neural network using stochastic gradient descent.</span>
<a name="line-226"></a><span class='hs-definition'>sgd</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>TrainingConfig</span>
<a name="line-227"></a>    <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Int</span>  <span class='hs-comment'>-- ^ epochs</span>
<a name="line-228"></a>    <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Int</span>  <span class='hs-comment'>-- ^ mini-batch size</span>
<a name="line-229"></a>    <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-conid'>Sample</span>
<a name="line-230"></a>    <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Network</span>
<a name="line-231"></a>    <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>IO</span> <span class='hs-conid'>Network</span>
<a name="line-232"></a><span class='hs-definition'>sgd</span> <span class='hs-varid'>trainingConfig</span> <span class='hs-varid'>epochs</span> <span class='hs-varid'>miniBatchSize</span> <span class='hs-varid'>trainingData</span> <span class='hs-varid'>network</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>go</span> <span class='hs-varid'>epochs</span> <span class='hs-varid'>network</span>
<a name="line-233"></a>    <span class='hs-keyword'>where</span> <span class='hs-varid'>go</span> <span class='hs-num'>0</span>     <span class='hs-varid'>net</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>putStrLn</span> <span class='hs-str'>"done!"</span> <span class='hs-varop'>&gt;&gt;</span> <span class='hs-varid'>pure</span> <span class='hs-varid'>net</span>
<a name="line-234"></a>          <span class='hs-varid'>go</span> <span class='hs-varid'>epoch</span> <span class='hs-varid'>net</span> <span class='hs-keyglyph'>=</span> <span class='hs-keyword'>do</span>
<a name="line-235"></a>              <span class='hs-varid'>putStrLn</span> <span class='hs-varop'>$</span> <span class='hs-str'>"Epoch: "</span> <span class='hs-varop'>++</span> <span class='hs-varid'>show</span> <span class='hs-varid'>epoch</span>
<a name="line-236"></a>              <span class='hs-varid'>shuffledTrainingData</span> <span class='hs-keyglyph'>&lt;-</span> <span class='hs-varid'>evalRandIO</span> <span class='hs-varop'>$</span> <span class='hs-varid'>vshuffle</span> <span class='hs-varid'>trainingData</span>
<a name="line-237"></a>              <span class='hs-keyword'>let</span> <span class='hs-varid'>miniBatches</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>miniBatchSize</span> <span class='hs-varop'>`vChunksOf`</span> <span class='hs-varid'>shuffledTrainingData</span>
<a name="line-238"></a>                  <span class='hs-varid'>net'</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>foldl'</span> <span class='hs-layout'>(</span><span class='hs-varid'>flip</span> <span class='hs-layout'>(</span><span class='hs-varid'>gradientDescentCore</span> <span class='hs-varid'>trainingConfig</span><span class='hs-layout'>)</span><span class='hs-layout'>)</span> <span class='hs-varid'>net</span> <span class='hs-varid'>miniBatches</span>
<a name="line-239"></a>              <span class='hs-varid'>go</span> <span class='hs-layout'>(</span><span class='hs-varid'>epoch</span><span class='hs-comment'>-</span><span class='hs-num'>1</span><span class='hs-layout'>)</span> <span class='hs-varid'>net'</span>
<a name="line-240"></a>
<a name="line-241"></a><a name="vChunksOf"></a><span class='hs-definition'>vChunksOf</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>Int</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-varid'>a</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-keyglyph'>[</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-varid'>a</span><span class='hs-keyglyph'>]</span>
<a name="line-242"></a><span class='hs-definition'>vChunksOf</span> <span class='hs-varid'>n</span> <span class='hs-varid'>vec</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Data</span><span class='hs-varop'>.</span><span class='hs-conid'>List</span><span class='hs-varop'>.</span><span class='hs-varid'>unfoldr</span> <span class='hs-varid'>makeChunk</span> <span class='hs-varid'>vec</span>
<a name="line-243"></a>    <span class='hs-keyword'>where</span> <span class='hs-varid'>makeChunk</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-varid'>a</span> <span class='hs-keyglyph'>-&gt;</span> <span class='hs-conid'>Maybe</span> <span class='hs-layout'>(</span><span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-varid'>a</span><span class='hs-layout'>,</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-conid'>Vector</span> <span class='hs-varid'>a</span><span class='hs-layout'>)</span>
<a name="line-244"></a>          <span class='hs-varid'>makeChunk</span> <span class='hs-varid'>v</span> <span class='hs-keyglyph'>|</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>null</span> <span class='hs-varid'>v</span>  <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Nothing</span>
<a name="line-245"></a>                      <span class='hs-keyglyph'>|</span> <span class='hs-varid'>otherwise</span> <span class='hs-keyglyph'>=</span> <span class='hs-conid'>Just</span> <span class='hs-varop'>$</span> <span class='hs-conid'>V</span><span class='hs-varop'>.</span><span class='hs-varid'>splitAt</span> <span class='hs-varid'>n</span> <span class='hs-varid'>v</span>
<a name="line-246"></a>
<a name="line-247"></a>
<a name="line-248"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-249"></a><span class='hs-comment'>-- Cost function derivatives.</span>
<a name="line-250"></a><span class='hs-comment'>-- ==================================================</span>
<a name="line-251"></a>
<a name="line-252"></a><a name="mse'"></a><span class='hs-comment'>-- | The derivative of the mean squared error cost function.</span>
<a name="line-253"></a><span class='hs-definition'>mse'</span> <span class='hs-keyglyph'>::</span> <span class='hs-conid'>CostDerivative</span>
<a name="line-254"></a><span class='hs-definition'>mse'</span> <span class='hs-varid'>outputActivations</span> <span class='hs-varid'>y</span> <span class='hs-keyglyph'>=</span> <span class='hs-varid'>outputActivations</span> <span class='hs-comment'>-</span> <span class='hs-varid'>y</span>
</pre></body>
</html>
