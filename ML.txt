-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Machine learning code.
--   
--   Please see README.md
@package ML
@version 0.1.0.0

module Control.Monad.Random.Extended
shuffle :: RandomGen g => [a] -> Rand g [a]
vshuffle :: RandomGen g => Vector a -> Rand g (Vector a)

module ML.Sample

-- | A training sample.
data Sample
Sample :: {-# UNPACK #-} !(Vector R) -> {-# UNPACK #-} !(Vector R) -> Sample
[sampleInput] :: Sample -> {-# UNPACK #-} !(Vector R)
[sampleExpectedOutput] :: Sample -> {-# UNPACK #-} !(Vector R)
instance GHC.Show.Show ML.Sample.Sample
instance GHC.Read.Read ML.Sample.Sample


-- | Provides a parser for text formatted MNIST data.
module ML.MnistLoader

-- | Parse the given bytestring into (training data, validation data, test
--   data).
--   
--   Data format:
--   
--   <ul>
--   <li>70000 lines lines of 785 doubles.</li>
--   <li>first 1 - 50000 lines are test data.</li>
--   <li>lines 50001 - 60000 are validation data.</li>
--   <li>lines 60001 - 70000 are test data.</li>
--   <li>first 784 double of each line are the image.</li>
--   <li>last double of each is the expected value.</li>
--   </ul>
parseMnist :: ByteString -> (Vector Sample, Vector Sample, Vector Sample)


-- | Type aliases for activation functions and their derivative.
module ML.NN.ActivationFunction

-- | An activation function for a neuron.
type ActivationFunction = R -> R

-- | The derivative of a neuron activation function.
type ActivationFunctionDerivative = R -> R


-- | Activation functions and their derivatives for use with neural
--   networks.
module ML.NN.Activation

-- | The sigmoid function:
--   <a>https://en.wikipedia.org/wiki/Sigmoid_function</a>.
sigmoid :: ActivationFunction

-- | The derivative of the sigmoid function.
sigmoid' :: ActivationFunctionDerivative


-- | Simple neural network implementation, for learning purposes only.
module ML.NN

-- | An activation function for a neuron.
type ActivationFunction = R -> R

-- | The derivative of a neuron activation function.
type ActivationFunctionDerivative = R -> R

-- | A layer in a neural network is a bias vector and weight matrix.
--   
--   Let <i>n</i> be the number of neurons in the layer and <i>m</i> be the
--   number of inputs to the layer. Then <a>layerBiases</a> is a vector in
--   Rⁿ and <a>layerWeights</a> is an <i>nxm</i> real matrix.
data Layer
Layer :: {-# UNPACK #-} !(Vector R) -> {-# UNPACK #-} !(Matrix R) -> Layer

-- | A vector in Rⁿ representing the neuron biases.
[layerBiases] :: Layer -> {-# UNPACK #-} !(Vector R)

-- | An <i>nxm</i> real matrix of the neuron weights.
[layerWeights] :: Layer -> {-# UNPACK #-} !(Matrix R)

-- | A network is a list of layers.
data Network
Network :: [Layer] -> Network
[networkLayers] :: Network -> [Layer]

-- | Generate a randomly initialized layer in a neural network.
randLayer :: RandomGen g => Int -> Int -> Rand g Layer

-- | Generate a random neural network given the size of each layer.
--   
--   For example, <tt>[3,2,4]</tt> will generate a neural network with 3
--   input neurons, 1 hidden layer with 2 neurons and 4 output neurons.
randNetwork :: RandomGen g => [Int] -> Rand g Network

-- | Feed the output from a previous layer to the next layer.
feedForward :: ActivationFunction -> Vector R -> Layer -> Vector R

-- | Run a neural network.
runNetwork :: Network -> ActivationFunction -> Vector R -> Vector R

-- | Configuration for training a network.
data TrainingConfig
TrainingConfig :: R -> ActivationFunction -> ActivationFunctionDerivative -> CostDerivative -> TrainingConfig

-- | η - Learning rate.
[trainingEta] :: TrainingConfig -> R
[trainingActivation] :: TrainingConfig -> ActivationFunction
[trainingActivationDerivative] :: TrainingConfig -> ActivationFunctionDerivative
[trainingCostDerivative] :: TrainingConfig -> CostDerivative

-- | A training sample.
data Sample
Sample :: {-# UNPACK #-} !(Vector R) -> {-# UNPACK #-} !(Vector R) -> Sample
[sampleInput] :: Sample -> {-# UNPACK #-} !(Vector R)
[sampleExpectedOutput] :: Sample -> {-# UNPACK #-} !(Vector R)

-- | The gradient of the network.
data Gradient
Gradient :: {-# UNPACK #-} !(Vector (Vector R)) -> {-# UNPACK #-} !(Vector (Matrix R)) -> Gradient
[gradientNablaB] :: Gradient -> {-# UNPACK #-} !(Vector (Vector R))
[gradientNablaW] :: Gradient -> {-# UNPACK #-} !(Vector (Matrix R))

-- | A vectorized function which returns ∂Cₓ/∂a.
--   
--   The first parameter is the output activation, the second parameter is
--   the expected output.
type CostDerivative = Vector R -> Vector R -> Vector R

-- | Train the neural network using stochastic gradient descent.
sgd :: TrainingConfig -> Int -> Int -> Vector Sample -> Network -> IO Network

-- | Update the network's weights and biases by applying gradient descent
--   for the given sample input.
gradientDescentCore :: TrainingConfig -> Vector Sample -> Network -> Network

-- | The derivative of the mean squared error cost function.
mse' :: CostDerivative

-- | Return z and activation values for each layer in the network.
--   
--   The values are returned in reverse order for use by the
--   backpropogation algorithm. We the State monad so it's more explicit
--   that the output activation of one layer is the input to the next.
computeZsAndAs :: ActivationFunction -> Network -> Vector R -> ([Vector R], [Vector R])
instance GHC.Show.Show ML.NN.Gradient
instance GHC.Read.Read ML.NN.Gradient
instance GHC.Read.Read ML.NN.Network
instance GHC.Show.Show ML.NN.Network
instance GHC.Read.Read ML.NN.Layer
instance GHC.Show.Show ML.NN.Layer
